{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "The goal is to assess various auto evaluation systems on the task of evaluating the quality of responses by utilizing a high quality reference answer.\n",
    "\n",
    "We have curated synthetic datasets for auto evaluation research in the following domain:\n",
    "- TruthfulQA\n",
    "- LegalBench\n",
    "- finance-alpaca\n",
    "\n",
    "The correct synthetic datasets are created by paraphrasing the original questions and answers without changing the meaning or any key information.\n",
    "\n",
    "The incorrect synthetic datasets are created by paraphrasing the original questions and answers with some changes in meaning or key information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep dataset for Auto Evaluation Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Paraphrase LLM Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAPHRASE_CORRECT_PROMPT = \"\"\"\n",
    "Please paraphrase the following sentence.\n",
    "\n",
    "Your paraphrased sentence should:\n",
    "• Retain the original meaning and essential information.\n",
    "• Be naturally written.\n",
    "• Use similar tone and style as the original text.\n",
    "• Be sufficiently different in wording from the original text while keeping the same meaning and essential information.\n",
    "• If the original text is very short or is a direct extraction (e.g., a brief phrase or quote or ), output the same text without changes.\n",
    "• Do not include any new information not present in the original text.\n",
    "\n",
    "\n",
    "You can introduce diversity through changes in diction, phrasing, sentence structure, formality, detail, and other stylistic elements.\n",
    "\n",
    "Original Text: {text}\n",
    "Paraphrased Text:\n",
    "\"\"\"\n",
    "\n",
    "PARAPHRASE_INCORRECT_PROMPT = \"\"\"\n",
    "Please paraphrase the following text in a way that is incorrect.\n",
    "\n",
    "Your paraphrased text can be incorrect in the following ways:\n",
    "• Retain only a portion of the original meaning and essential information. A portion of the original meaning and essential information should be missing.\n",
    "• Sometimes include new information that is not present in the original text.\n",
    "\n",
    "The paraphrase should:\n",
    "• Be naturally written.\n",
    "• Use similar tone and style as the original text.\n",
    "\n",
    "You can introduce diversity through changes in diction, phrasing, sentence structure, formality, detail, and other stylistic elements.\n",
    "\n",
    "Original Text: {text}\n",
    "Paraphrased Text:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import random\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import os\n",
    "\n",
    "def get_gpt4_paraphrase(text, prompt):\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that paraphrases and rewrites text as instructed by the user.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt.format(text=text)}\n",
    "            ]\n",
    "        )\n",
    "        print(\"Reference Response: \", text)\n",
    "        print(\"Paraphrased Response: \", response.choices[0].message.content.strip())\n",
    "        print(\"-\"*100, \"\\n\")\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in GPT-4 API call: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_row(item, response_field, prompt):\n",
    "    index, row = item\n",
    "    paraphrased_text = get_gpt4_paraphrase(row[response_field], prompt)\n",
    "    return paraphrased_text\n",
    "\n",
    "def parallel_paraphrase(df, response_field, num_workers=None, prompt=PARAPHRASE_CORRECT_PROMPT):\n",
    "    if num_workers is None:\n",
    "        num_workers = os.cpu_count()\n",
    "    print(f\"Number of workers: {num_workers}\")\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        results = list(tqdm(executor.map(lambda x: process_row(x, response_field, prompt), df.iterrows()), total=len(df), desc=\"Paraphrasing\"))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARQUET_PATH = \"/Users/minhajul/personal/github/ide_2.0/storage/qna_data/original_truthful_qa/truthful_qa.parquet\"\n",
    "df = pd.read_parquet(PARQUET_PATH)\n",
    "df = df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrased_results = parallel_paraphrase(df, response_field='best_answer')\n",
    "\n",
    "# Sort the results by index to ensure correct alignment\n",
    "paraphrased_results.sort(key=lambda x: x[0])\n",
    "\n",
    "# Add the paraphrased texts to the dataframe\n",
    "df['paraphrased_text'] = [result[1] for result in paraphrased_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare TruthfulQA Auto Evaluation Dataset\n",
    "https://huggingface.co/datasets/truthfulqa/truthful_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARQUET_PATH = \"../storage/qna_data/original_truthful_qa/truthful_qa.parquet\"\n",
    "df = pd.read_parquet(PARQUET_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_gpt4_paraphrase(text):\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that paraphrases and rewrite text. Keep the essential information and do not add any additional information.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Paraphrase and rewrite the following text: {text}\"}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in GPT-4 API call: {e}\")\n",
    "        return None\n",
    "\n",
    "new_df = pd.DataFrame(columns=['question', 'reference_response', 'new_response', 'result'])\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\"):\n",
    "    # Add row with correct answer\n",
    "    correct_answer = random.choice(row['correct_answers']) if row['correct_answers'].any() else None\n",
    "    new_df = pd.concat([new_df, pd.DataFrame({\n",
    "        'question': [row['question']],\n",
    "        'reference_response': [row['best_answer']],\n",
    "        'new_response': [correct_answer],\n",
    "        'result': ['REJECT']\n",
    "    })], ignore_index=True)\n",
    "    \n",
    "    # Add row with incorrect answer\n",
    "    incorrect_answer = random.choice(row['incorrect_answers']) if row['incorrect_answers'].any() else None\n",
    "    new_df = pd.concat([new_df, pd.DataFrame({\n",
    "        'question': [row['question']],\n",
    "        'reference_response': [row['best_answer']],\n",
    "        'new_response': [incorrect_answer],\n",
    "        'result': ['REJECT']\n",
    "    })], ignore_index=True)\n",
    "    \n",
    "    # Get GPT-4 paraphrase of best_answer and add new row\n",
    "    paraphrased_answer = get_gpt4_paraphrase(row['best_answer'])\n",
    "    if paraphrased_answer:\n",
    "        new_df = pd.concat([new_df, pd.DataFrame({\n",
    "            'question': [row['question']],\n",
    "            'reference_response': [row['best_answer']],\n",
    "            'new_response': [paraphrased_answer],\n",
    "            'result': ['ACCEPT']\n",
    "        })], ignore_index=True)\n",
    "\n",
    "# Display the dataframe\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates where reference_response and new_response are the same\n",
    "new_df = new_df[new_df['reference_response'] != new_df['new_response']]\n",
    "\n",
    "# Reset the index after removing duplicates\n",
    "new_df = new_df.reset_index(drop=True)\n",
    "\n",
    "# Display the shape of the dataframe after removing duplicates\n",
    "print(\"Shape after removing duplicates:\", new_df.shape)\n",
    "\n",
    "# Optionally, save the new dataframe to a CSV file\n",
    "new_df.to_parquet(\"../storage/auto_eval_research/truthful_qa/truthful_qa_eval.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare LegalBench Auto Evaluation Dataset\n",
    "https://huggingface.co/datasets/nguha/legalbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LegalBench dataset from Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "legalbench_dataset = load_dataset(\"nguha/legalbench\", \"rule_qa\")\n",
    "\n",
    "# Print the dataset info\n",
    "print(legalbench_dataset)\n",
    "\n",
    "# Access the 'test' split (assuming it exists)\n",
    "test_data = legalbench_dataset['test']\n",
    "\n",
    "# Display the first few examples\n",
    "print(test_data[:5])\n",
    "\n",
    "# Get the column names\n",
    "print(\"Columns:\", test_data.column_names)\n",
    "\n",
    "# Get the number of examples\n",
    "print(\"Number of examples:\", len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 5 rows of the test data in a readable format\n",
    "for i, row in enumerate(test_data[:5]):\n",
    "    print(f\"Row {i+1}:\")\n",
    "    print(f\"  index: {test_data['index'][i]}\")\n",
    "    print(f\"  text: {test_data['text'][i]}\")\n",
    "    print(f\"  answer: {test_data['answer'][i]}\")\n",
    "    print(f\"  doctrine: {test_data['doctrine'][i]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paraphrase the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_bench_df = test_data.to_pandas()\n",
    "legal_bench_df.rename(columns={'text': 'question', 'answer': 'reference_response'}, inplace=True)\n",
    "legal_bench_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate paraphrases\n",
    "def generate_paraphrases(df, response_field):\n",
    "    print(\"Generating correct paraphrases...\")\n",
    "    df['correct_paraphrase'] = parallel_paraphrase(df, response_field, prompt=PARAPHRASE_CORRECT_PROMPT)\n",
    "    \n",
    "    print(\"-\"*100, \"\\n\")\n",
    "    print(\"Generating incorrect paraphrases...\")\n",
    "    df['incorrect_paraphrase'] = parallel_paraphrase(df, response_field, prompt=PARAPHRASE_INCORRECT_PROMPT)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create final dataframe\n",
    "def create_final_df(df):\n",
    "    correct_df = df.assign(new_response=df['correct_paraphrase'], result='ACCEPT')\n",
    "    incorrect_df = df.assign(new_response=df['incorrect_paraphrase'], result='REJECT')\n",
    "    \n",
    "    final_df = pd.concat([correct_df, incorrect_df], ignore_index=True)\n",
    "    final_df = final_df[['question', 'reference_response', 'new_response', 'doctrine', 'result']]\n",
    "    \n",
    "    return final_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Main process\n",
    "legal_bench_df = generate_paraphrases(legal_bench_df, 'reference_response')\n",
    "final_df = create_final_df(legal_bench_df)\n",
    "\n",
    "print(f\"Final dataframe shape: {final_df.shape}\")\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, save the new dataframe to a CSV file\n",
    "final_df.to_parquet(\"../storage/auto_eval_research/legal_bench/legal_bench_eval.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare gbharti/finance-alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LegalBench dataset from Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset with a subset of 5000 examples\n",
    "finance_alpaca_dataset = load_dataset(\"gbharti/finance-alpaca\", split=\"train[:5000]\")\n",
    "\n",
    "# Print the dataset info\n",
    "print(finance_alpaca_dataset)\n",
    "\n",
    "# Display the first few examples\n",
    "print(finance_alpaca_dataset[:5])\n",
    "\n",
    "# Get the column names\n",
    "print(\"Columns:\", finance_alpaca_dataset.column_names)\n",
    "\n",
    "# Get the number of examples\n",
    "print(\"Number of examples:\", len(finance_alpaca_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 5 rows of the test data in a readable format\n",
    "for i in range(5):\n",
    "    print(f\"Row {i+1}:\")\n",
    "    print(f\"  instruction: {finance_alpaca_dataset['instruction'][i]}\")\n",
    "    print(f\"  output: {finance_alpaca_dataset['output'][i]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paraphrase the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_alpaca_df  = finance_alpaca_dataset.to_pandas() \n",
    "finance_alpaca_df.rename(columns={'instruction': 'question', 'output': 'reference_response'}, inplace=True)\n",
    "finance_alpaca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a subset\n",
    "finance_alpaca_df = finance_alpaca_df.head(500)\n",
    "finance_alpaca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate paraphrases\n",
    "def generate_paraphrases(df, response_field):\n",
    "    print(\"Generating correct paraphrases...\")\n",
    "    df['correct_paraphrase'] = parallel_paraphrase(df, response_field, prompt=PARAPHRASE_CORRECT_PROMPT)\n",
    "    \n",
    "    print(\"-\"*100, \"\\n\")\n",
    "    print(\"Generating incorrect paraphrases...\")\n",
    "    df['incorrect_paraphrase'] = parallel_paraphrase(df, response_field, prompt=PARAPHRASE_INCORRECT_PROMPT)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create final dataframe\n",
    "def create_final_df(df):\n",
    "    correct_df = df.assign(new_response=df['correct_paraphrase'], result='ACCEPT')\n",
    "    incorrect_df = df.assign(new_response=df['incorrect_paraphrase'], result='REJECT')\n",
    "    \n",
    "    final_df = pd.concat([correct_df, incorrect_df], ignore_index=True)\n",
    "    final_df = final_df[['question', 'reference_response', 'new_response', 'result']]\n",
    "    \n",
    "    return final_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Main process\n",
    "finance_alpaca_df = generate_paraphrases(finance_alpaca_df, 'reference_response')\n",
    "final_df = create_final_df(finance_alpaca_df)\n",
    "\n",
    "print(f\"Final dataframe shape: {final_df.shape}\")\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, save the new dataframe to a CSV file\n",
    "final_df.to_parquet(\"../storage/auto_eval_research/finance_alpaca/finance_alpaca_eval.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ide2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
